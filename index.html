<!doctype html><html lang="en"><head><meta charset="UTF-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Tippy Project</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-156189593-1"></script><script>window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());

  gtag("config", "UA-156189593-1");</script><script defer="defer" src="index.0f2d2079e1fc796c848c.js"></script></head><body><nav><ul class="nav-menu"><li class="nav-logo"><a href="index.html">Tippy Project</a></li><li class="nav-item"><a href="cmaes-demo.html">Optimizer Demo</a></li><li class="nav-item"><a href="spot.html">Complex Limbs</a></li><li class="nav-item"><a href="tippy.html">Interactive Agent</a></li><li class="nav-toggle"><a href="#"><i class="fas fa-bars"></i></a></li></ul></nav><main><header><h1 class="page-title">Tippy Project</h1><p class="subtitle">Interactive Intelligent Agents</p><video class="video-demo" controls autoplay loop><source src="tippy-demo.mp4" type="video/mp4"/><source src="tippy-demo.webm" type="video/webm"/><p>Sorry, your browser doesn't support HTML5 video.</p></video><summary><b>Summary:</b> Autonomous intelligent agent simulations are abundant, but users can rarely <em>interact</em> with the trained agent. The Tippy Project is my attempt to simulate robots that users can play with.</summary></header><section><h2 class="section-head">Site Overview</h2><ol><li><a href="cmaes-demo.html">Optimizer Demo</a> - What is CMA-ES and how does it work?</li><li><a href="spot.html">Complex Limbs</a> - Watch a robot learn to run</li><li><a href="tippy.html">Interactive Agent</a> - Control a robot in the browser</li></ol></section><section><h2 class="section-head">Goals</h2><ul><li>Simulate a neural network controlled robot that follows user inputs</li><li>Allow users to train their own agents client-side in the browser</li><li>Implement the CMA-ES algorithm from scratch</li><li>Make a website to share the process</li></ul></section><section><h2 class="section-head">Interaction</h2><p>An intelligent agent is simple: it perceives its <b>environment</b>, takes <b>actions</b>, and receives <b>rewards</b> based on those actions. To Tippy, the <b>human user</b> is just another aspect of its environment. User inputs are no different from the height of the ground or body angle from Tippy's perspective. To distinguish user inputs I designed the reward (or objective) function to depend on those inputs. <em>The reward function evaluates the agent's actions relative to whatever command the user is sending.</em></p><p>Because training takes place over hundreds of episodes, I <b>simulated random user inputs</b>, and the quality of neural network weights were evaluated based on how accurately Tippy followed those simulated commands. Because some set of weights might happen to randomly excel at a particular sequence of commands (causing overfitting) each solution was evaluated as <b>the mean accuracy to a set of random commands</b> that exhibited a range of "twitchiness". Random variation between training episodes reduces overfitting but also slows the learning process. To increase consistency in the reward function, each episode includes a sequence of commands to remain motionless and also to move right as quickly as possible.</p><p>One trick that dramatically increased the useability of the controls was <b>forcing symmetry</b> on the control system. Positive and negative directional inputs with the same magnitude are identical to Tippy; the other x-axis inputs are simply reversed (the <em>sign</em> of x velocity or angular velocity is flipped, while the <em>order</em> of the sensor inputs is reversed, right to left or left to right). This forced symmetry <b>accelerates training</b> by reducing the space of behaviors tippy must learn and <b>increases the predictability</b> of Tippy's behavior. The only drawback is the tendency for a discontinuity in behavior when inputs transition from positive to negative, that is, if Tippy's control system responds very differently to the "flipping" inputs (x-velocity, etc) as the user input passes from positive to negative.</p><p>Speaking of controls, <b>the user inputs were restricted</b> so that Tippy would be capable of accurately following a random set of commands. It is not physically realistic for Tippy to change direction instantly, so the controls are restricted to enforce smooth transitions. Tippy is an <b>inverted pendulum</b>. Tippy's acceleration is proportional to the sine of its body angle. So I first modeled the user inputs as request for a given body angle. This forces the user to think in terms of <b>accelerating</b> Tippy rather than requesting a particular <b>velocity</b>. This was effective for early training because Tippy's target behavior directly corresponds to one of the inputs it receives (body angle). It was easy for a simple neural network to tend toward that input, almost thermostatically. Unfortunately, acceleration-based controls do not have a natural upper limit, so users could request Tippy to continue accelerating to unstable speeds. In addition, when the user requests zero-acceleration (no body angle), the squared-error can be small (a nearly-zero body angle) while the impact on physical behavior, <b>a small "drifting" acceleration </b>, is extremely distracting. As a human user, I find velocity-based behavior much more natural than acceleration-based. (Ice skaters may feel differently.) acc</p></section></main><footer><ul><li><a href="https://github.com/taslater/tippy-project">Github repository</a></li></ul></footer></body></html>